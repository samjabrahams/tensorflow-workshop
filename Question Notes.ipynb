{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How to train a new layer on trained networks (cats and dogs Kaggle competition where certain participants used pre-trained networks and simply added a final training layer)\n",
    "\n",
    "2. Regression vs. classification in TF\n",
    "\n",
    "3. Best optimizers for different types of tasks (Lagrangean)\n",
    "\n",
    "4. Avoiding non-convergence of validation loss\n",
    "\n",
    "5. How much data should I need?/how to make the most of a small dataset\n",
    "\n",
    "6. How to pick number of layers/neurons\n",
    "\n",
    "7. Merits of different activation functions\n",
    "\n",
    "8. Evaluating your NN- training and validation RMSE or Accuracy\n",
    "\n",
    "9. How to run train TF on large datasets using batches to minimize the Low Memory errors?\n",
    "\n",
    "10. Is there a pre-trained CNN on TF using large datasets? If yes how it can be used for fast image retrieval?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Using pre-trained networks\n",
    "\n",
    "The lecture will have a lot of this!\n",
    "\n",
    "## 2: Regression vs Classification\n",
    "\n",
    "It's defined by hand, so it's up to you to use an appropriate output and error functions. \n",
    "\n",
    "For neural networks, the softmax function is what converts \"scores\" into a pseudo-percentage for classification. You can have the final layer be the numeric scores instead and use mean squared error. That said, for regression problems it's worth evaluating the benefit of any gained accuracy versus your ability to analyze the model for causality. \n",
    "\n",
    "## 3: Optimizers\n",
    "\n",
    "With regards to Lagrangean optimization: TensorFlow generally isn't the best tool for constrained optimization problems- software like MatLab/Octave will be better bets for that.\n",
    "\n",
    "That said, we do have options for gradient descent optimization techniques that incorporate really clever optimization techniques (and were developed by people much smarter than myself!). \n",
    "\n",
    "* Standard gradient descent\n",
    "* Momentum\n",
    "* RMSProp\n",
    "* AdaDelta\n",
    "* Adam\n",
    "\n",
    "[Awesome article discussing the math behind the various gradient descent algorithms](http://sebastianruder.com/optimizing-gradient-descent/)\n",
    "\n",
    "[Good StackOverflow discussion of gradient descent and Lagrangian multipliers](http://stackoverflow.com/questions/12284638/gradient-descent-with-constraints-lagrange-multipliers?rq=1)\n",
    "\n",
    "## 4: Non-convergence in loss/activations\n",
    "\n",
    "Ways you are likely to see non-convergence:\n",
    "\n",
    "* Learning rate is too high, causing descent to be unstable\n",
    "    * Reduce learning rate\n",
    "* Accidental `log(0)`\n",
    "    * Use `tf.clip_by_value()` on inputs to `tf.log()` to prevent value going to zero\n",
    "* Huge layers, net value passed to next layer adds up to ridiculously huge amounts, get passed on to next layer with ReLU\n",
    "    * Reduce the magnitude of initial weight values\n",
    "    * Normalize inputs to be between [-1, 1], mean zero\n",
    "* Get sample values of neurons layer by layer to help debug exactly where the divergence is occurring\n",
    "\n",
    "## 5: How much data? How to make most of data?\n",
    "\n",
    "As much data as possible! With some types of data, there isn't much you can do to generate more data (if you adjust the values of some of the inputs, you can't definitively say what the output label would be). That said, with something like labeled _image_ data, there's a fair amount you can do to multiply your effective training size. You can flip, scale, skew (a little bit), adjust brightness/contrast, crop, and add noise- randomly. Just because you adjust certain features of the image doesn't mean your cat picture isn't still a cat picture!\n",
    "\n",
    "## 6: How to pick numbers of neurons/layers\n",
    "\n",
    "This is probably the most common question people have when learning about neural networks. Unfortunately, there isn't a clear cut answer to this, and it still requires a bit of manual fiddling and some intuition. In general, here are the tips I've picked up along the way:\n",
    "\n",
    "1. The number of neurons in any layer shouldn't be any higher than 10x the previous layer (absolute max is 10x)\n",
    "2. Start with a few layers.\n",
    "3. Add layers/neurons until you overfit\n",
    "4. When you overfit, add regularization techniques, such as dropout and L2 regularization\n",
    "\n",
    "\n",
    "## 7: Activation functions\n",
    "\n",
    "Brief overview of the three most common types of activation functions you'll see:\n",
    "\n",
    "* Sigmoid/logistic:\n",
    "    * These days, the sigmoid is generally reserved for the memory cell units in long short-term memory (LSTM) networks. It used to be the most common activation function, and is usually the first one people learn about.\n",
    "* Hyperbolic tangent (tanh)\n",
    "    * Is now the drop-in replacement for Sigmoid in any use case where the bounds (0,1) aren't required. Its symmetry about 0 is useful \n",
    "* Rectified linear unit (ReLU)\n",
    "    * Has multiple nice properties. It helps prevent the so-called \"vanishing gradient\" problem, as it doesn't compress the values passed into it. Additionally, the hard-zero cutoff means that it's more likely to cause sparse tensors to be passed along. They are used extensively in larger convolutional networks. \n",
    "   \n",
    "All of these functions have easy-to-compute derivatives, assuming you already have their output value. My recommendation is to look at existing models to get a feel for when to use one activation function versus another!\n",
    "\n",
    "## 8: Evaluating your neural network\n",
    "\n",
    "Depends on what you mean by \"evaluating\". When doing gradient descent, you won't ever use as a target. But after your network finishes training and you want to compare how well it does, there are generally a few different ways to rate your model. It depends on what your model is doing: if you are detecting cancer, you'd much rather have false-positives than false-negatives (which would leave the cancer untreated!), but if you are trying to make a purchase decision, it's probably better to have a more conservative model.\n",
    "\n",
    "In terms of just finding a target for descent, you may use RMSE, cross-entropy, some sort of co-occurrence relation- it depends on the specific task.\n",
    "\n",
    "## 9: Avoiding loading huge datasets into memory\n",
    "\n",
    "Good question! There are two main ways to go about doing this in TensorFlow:\n",
    "\n",
    "* Manually pass in filenames for TensorFlow to open as you do your batches\n",
    "* Create a `tf.Queue`, which automatically loads in a certain capacity of data in the background asynchronously\n",
    "\n",
    "I wanted to show you guys how to create a file reader queue today, but it had to get cut due to time constraints and the fact that certain features don't play nice across multiple Jupyter Notebooks. Do check out the [reading data](https://www.tensorflow.org/programmers_guide/reading_data) and [threading/queues tutorials](https://www.tensorflow.org/programmers_guide/threading_and_queues) on tensorflow.org.\n",
    "\n",
    "## 10: Pre-trained networks for image retrieval\n",
    "\n",
    "Yes, there are pre-trained networks, and using them for straight-forward image retrieval is part of this class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
