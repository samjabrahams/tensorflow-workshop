{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python > 3, TensorFlow > 1.0\n",
    "# Import core TensorFlow modules\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modules required for file download and extraction\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's notMNIST! Don't worry, we won't be here for long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directory to download dataset\n",
    "DATASET_DIR = 'data/notmnist/'\n",
    "\n",
    "# Directory to save TensorBoard summary statistics, graph data, etc\n",
    "TB_DIR = 'tensorboard/feedforward'\n",
    "\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "    os.mkdir(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download(filename, url, force=False):\n",
    "    \"\"\"Download a file if not present.\"\"\"\n",
    "    if force or not os.path.exists(DATASET_DIR + filename):\n",
    "        filename, _ = urlretrieve(url + filename, DATASET_DIR + filename)\n",
    "        print('\\nDownload complete for {}'.format(filename))\n",
    "        return filename\n",
    "    else:\n",
    "        print('File {} already present.'.format(filename))\n",
    "    return DATASET_DIR + filename\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    if os.path.isdir(root) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('{} already present - don\\'t need to extract {}.'.format(root, filename))\n",
    "    else:\n",
    "        print('Extracting data for {}. This may take a while. Please wait.'.format(root))\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(root[0:root.rfind('/') + 1])\n",
    "        tar.close()\n",
    "    data_folders = [\n",
    "        os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "        if os.path.isdir(os.path.join(root, d))]\n",
    "    print(data_folders)\n",
    "    return data_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Locations to download data:\n",
    "url = 'http://yaroslavvb.com/upload/notMNIST/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File notMNIST_small.tar.gz already present.\n"
     ]
    }
   ],
   "source": [
    "# Download notMNIST small dataset\n",
    "train_zip_path = maybe_download('notMNIST_small.tar.gz', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/notmnist/notMNIST_small already present - don't need to extract data/notmnist/notMNIST_small.tar.gz.\n",
      "['data/notmnist/notMNIST_small/A', 'data/notmnist/notMNIST_small/B', 'data/notmnist/notMNIST_small/C', 'data/notmnist/notMNIST_small/D', 'data/notmnist/notMNIST_small/E', 'data/notmnist/notMNIST_small/F', 'data/notmnist/notMNIST_small/G', 'data/notmnist/notMNIST_small/H', 'data/notmnist/notMNIST_small/I', 'data/notmnist/notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "# Extract datasets\n",
    "train_folders = maybe_extract(train_zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_height = 28  # Pixel height of images\n",
    "image_width = 28  # Pixel width of images\n",
    "pixel_depth = 255.0  # Number of levels per pixel\n",
    "expected_img_shape = (image_height, image_width)  # Black and white image, no 3rd dimension\n",
    "num_labels = len(train_folders)\n",
    "\n",
    "def load_image_folder(folder):\n",
    "    \"\"\"Load the data for a single image label.\"\"\"\n",
    "\n",
    "    # Create a list of image paths inside the folder  \n",
    "    image_files = os.listdir(folder)\n",
    "    # Create empty numpy array to hold data\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_height, image_width),\n",
    "                         dtype=np.float32)\n",
    "    num_images = 0  # Counter for number of successful images loaded\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            # Read in image pixel data as floating point values\n",
    "            image_data = ndimage.imread(image_file).astype(float)\n",
    "            # Scale values: [0.0, 255.0] => [-1.0, 1.0] \n",
    "            image_data = (image_data - pixel_depth / 2) / (pixel_depth / 2)\n",
    "            if image_data.shape != expected_img_shape:\n",
    "                print('File {} has unexpected dimensions: '.format(str(image_data.shape)))\n",
    "                continue\n",
    "            # Add image to the numpy array dataset\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, '- skipping this file and moving on.')\n",
    "  \n",
    "    # Trim dataset to remove unused space\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_data_label_arrays(num_rows, image_height, image_width):\n",
    "    \"\"\"\n",
    "    Creates and returns empty numpy arrays for input data and labels\n",
    "    \"\"\"\n",
    "    if num_rows:\n",
    "        dataset = np.ndarray((num_rows, image_height, image_width), dtype=np.float32)\n",
    "        labels = np.ndarray(num_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels\n",
    "\n",
    "def collect_datasets(data_folders):\n",
    "    datasets = []\n",
    "    total_images = 0\n",
    "    for label, data_folder in enumerate(data_folders):\n",
    "        # Bring all test folder images in as numpy arrays\n",
    "        dataset = load_image_folder(data_folder)\n",
    "        num_images = len(dataset)\n",
    "        total_images += num_images\n",
    "        datasets.append((dataset, label, num_images))\n",
    "    return datasets, total_images\n",
    "\n",
    "def merge_train_test_datasets(datasets, total_images, percent_test):\n",
    "    num_train = int(total_images * (1.0 - percent_test))\n",
    "    num_test = int(total_images * percent_test)\n",
    "    train_dataset, train_labels = make_data_label_arrays(num_train, image_height, image_width)\n",
    "    test_dataset, test_labels = make_data_label_arrays(num_test, image_height, image_width)\n",
    "    \n",
    "    train_counter = 0\n",
    "    test_counter = 0\n",
    "    dataset_counter = 1\n",
    "    for dataset, label, num_images in datasets:\n",
    "        np.random.shuffle(dataset)\n",
    "        if dataset_counter != len(datasets):\n",
    "            n_v = int(num_images // (1.0 / percent_test))\n",
    "            n_t = num_images - n_v\n",
    "        else:\n",
    "            # Last label, make sure dataset sizes match up to what we created\n",
    "            n_v = len(test_dataset) - test_counter\n",
    "            n_t = len(train_dataset) - train_counter\n",
    "        train_dataset[train_counter: train_counter + n_t] = dataset[:n_t]\n",
    "        train_labels[train_counter: train_counter + n_t] = label\n",
    "        test_dataset[test_counter: test_counter + n_v] = dataset[n_t: n_t + n_v]\n",
    "        test_labels[test_counter: test_counter + n_v] = label\n",
    "        train_counter += n_t\n",
    "        test_counter += n_v\n",
    "        dataset_counter += 1\n",
    "    return train_dataset, train_labels, test_dataset, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not read: data/notmnist/notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file 'data/notmnist/notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - skipping this file and moving on.\n",
      "Could not read: data/notmnist/notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file 'data/notmnist/notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png' - skipping this file and moving on.\n"
     ]
    }
   ],
   "source": [
    "train_test_datasets, train_test_total_images = collect_datasets(train_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_labels, test_dataset, test_labels = \\\n",
    "  merge_train_test_datasets(train_test_datasets, train_test_total_images, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16851"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert (height, width, channels) to vector\n",
    "\n",
    "```\n",
    "Previous example shape:\n",
    "(10, 10, 3)\n",
    "\n",
    "New vector:\n",
    "[300]\n",
    "```\n",
    "\n",
    "# Convert integer label with one-hot vector\n",
    "\n",
    "```\n",
    "Previous label:\n",
    "4\n",
    "\n",
    "New one-hot vector (assuming 10 labels)\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert data examples flattened vectors\n",
    "# Convert labels to one-hot encoding\n",
    "num_channels = 1  # grayscale\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape( (-1, image_height * image_width)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (16851, 784) (16851, 10)\n",
      "Test set (1872, 784) (1872, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle data/labels in unison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_data_with_labels(dataset, labels):\n",
    "    indices = range(len(dataset))\n",
    "    np.random.shuffle(np.copy(indices))\n",
    "    new_data = np.ndarray(dataset.shape, dataset.dtype)\n",
    "    new_labels = np.ndarray(labels.shape, dataset.dtype)\n",
    "    n = 0\n",
    "    for i in indices:\n",
    "        new_data[n] = dataset[i]\n",
    "        new_labels[n] = labels[i]\n",
    "        n += 1\n",
    "    return new_data, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_labels = shuffle_data_with_labels(train_dataset, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually create our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "num_hidden_1 = 300\n",
    "num_hidden_2 = 150\n",
    "num_hidden_3 = 50\n",
    "\n",
    "# Input data.\n",
    "input_data = tf.placeholder(tf.float32, shape=(None, image_height * image_width), name=\"input_data\")\n",
    "input_labels = tf.placeholder(tf.float32, shape=(None, num_labels), name=\"input_labels\")\n",
    "  \n",
    "with tf.name_scope('hidden1'):\n",
    "    weights = tf.Variable(tf.truncated_normal([image_height * image_width, num_hidden_1]), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([num_hidden_1]), name='biases')\n",
    "    hidden1 = tf.nn.sigmoid(tf.matmul(input_data, weights) + biases)\n",
    "\n",
    "with tf.name_scope('hidden2'):\n",
    "    weights = tf.Variable(tf.truncated_normal([num_hidden_1, num_hidden_2]), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([num_hidden_2]), name='biases')\n",
    "    hidden2 = tf.nn.sigmoid(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "with tf.name_scope('hidden3'):\n",
    "    weights = tf.Variable(tf.truncated_normal([num_hidden_2, num_hidden_3]), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([num_hidden_3]), name='biases')\n",
    "    hidden3 = tf.nn.sigmoid(tf.matmul(hidden2, weights) + biases)\n",
    "\n",
    "with tf.name_scope('output_layer'):\n",
    "    weights = tf.Variable(tf.truncated_normal([num_hidden_3, num_labels]), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([num_labels]), name='biases')\n",
    "    logits = tf.matmul(hidden3, weights) + biases\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_labels))\n",
    "    \n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "# Predictions for the training and test data.\n",
    "model_prediction = tf.nn.softmax(logits, name=\"prediction\")\n",
    "label_prediction = tf.argmax(model_prediction, 1, name=\"predicted_label\")\n",
    "   \n",
    "# Global step\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "inc_step = global_step.assign_add(1)\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "# TF API change for v1.0 => summary.scalar: name must be python string, placeholders not allowed\n",
    "#    summary_label = tf.placeholder(tf.string, []) \n",
    "#    tf.summary.scalar(tf.reduce_join([b'loss_', summary_label], 0),loss)\n",
    "    tf.summary.scalar('loss',loss)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(label_prediction, tf.argmax(input_labels, 1))\n",
    "        model_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#        acc_summ = tf.summary.scalar(tf.reduce_join([b'accuracy_', summary_label], 0), model_accuracy)\n",
    "        acc_summ = tf.summary.scalar('accuracy', model_accuracy)\n",
    "    \n",
    "merged_summaries = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now run the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "writer = tf.summary.FileWriter(TB_DIR, graph=session.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 50: 0.033071\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 100: 0.036841\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 150: 0.086455\n",
      "Minibatch accuracy: 0.96875\n",
      "Minibatch loss at step 200: 0.082927\n",
      "Minibatch accuracy: 1.0\n",
      "Test accuracy: 0.10576923191547394\n",
      "Minibatch loss at step 250: 0.104419\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 300: 0.159961\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 350: 0.280118\n",
      "Minibatch accuracy: 0.921875\n",
      "Minibatch loss at step 400: 0.514911\n",
      "Minibatch accuracy: 0.875\n",
      "Test accuracy: 0.11057692021131516\n",
      "Minibatch loss at step 450: 2.073001\n",
      "Minibatch accuracy: 0.28125\n",
      "Minibatch loss at step 500: 4.284349\n",
      "Minibatch accuracy: 0.3125\n",
      "Minibatch loss at step 550: 0.036258\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 600: 0.036082\n",
      "Minibatch accuracy: 1.0\n",
      "Test accuracy: 0.10630341619253159\n",
      "Minibatch loss at step 650: 0.073983\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 700: 0.089361\n",
      "Minibatch accuracy: 0.984375\n",
      "Minibatch loss at step 750: 0.097272\n",
      "Minibatch accuracy: 0.984375\n",
      "Minibatch loss at step 800: 0.090528\n",
      "Minibatch accuracy: 0.984375\n",
      "Test accuracy: 0.14049145579338074\n",
      "Minibatch loss at step 850: 0.164613\n",
      "Minibatch accuracy: 0.984375\n",
      "Minibatch loss at step 900: 0.259735\n",
      "Minibatch accuracy: 0.953125\n",
      "Minibatch loss at step 950: 0.694470\n",
      "Minibatch accuracy: 0.78125\n",
      "Minibatch loss at step 1000: 2.157603\n",
      "Minibatch accuracy: 0.0625\n",
      "Test accuracy: 0.18910256028175354\n",
      "Minibatch loss at step 1050: 0.038159\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 1100: 0.036967\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 1150: 0.053373\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 1200: 0.038679\n",
      "Minibatch accuracy: 1.0\n",
      "Test accuracy: 0.11645299196243286\n",
      "Minibatch loss at step 1250: 0.103063\n",
      "Minibatch accuracy: 0.984375\n",
      "Minibatch loss at step 1300: 0.125707\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 1350: 0.094113\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 1400: 0.125491\n",
      "Minibatch accuracy: 0.984375\n",
      "Test accuracy: 0.16132478415966034\n",
      "Minibatch loss at step 1450: 0.296819\n",
      "Minibatch accuracy: 0.984375\n",
      "Minibatch loss at step 1500: 0.663421\n",
      "Minibatch accuracy: 0.84375\n",
      "Minibatch loss at step 1550: 3.440672\n",
      "Minibatch accuracy: 0.015625\n",
      "Minibatch loss at step 1600: 0.101984\n",
      "Minibatch accuracy: 0.984375\n",
      "Test accuracy: 0.14423076808452606\n",
      "Minibatch loss at step 1650: 0.058319\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 1700: 0.059173\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 1750: 0.139425\n",
      "Minibatch accuracy: 0.96875\n",
      "Minibatch loss at step 1800: 0.108142\n",
      "Minibatch accuracy: 0.953125\n",
      "Test accuracy: 0.21207265555858612\n",
      "Minibatch loss at step 1850: 0.079794\n",
      "Minibatch accuracy: 0.984375\n",
      "Minibatch loss at step 1900: 0.105759\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 1950: 0.209630\n",
      "Minibatch accuracy: 0.984375\n",
      "Minibatch loss at step 2000: 0.723024\n",
      "Minibatch accuracy: 0.765625\n",
      "Test accuracy: 0.2441239356994629\n",
      "Minibatch loss at step 2050: 1.098320\n",
      "Minibatch accuracy: 0.6875\n",
      "Minibatch loss at step 2100: 3.906893\n",
      "Minibatch accuracy: 0.015625\n",
      "Minibatch loss at step 2150: 0.048850\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 2200: 0.059928\n",
      "Minibatch accuracy: 1.0\n",
      "Test accuracy: 0.17200854420661926\n",
      "Minibatch loss at step 2250: 0.059578\n",
      "Minibatch accuracy: 0.984375\n",
      "Minibatch loss at step 2300: 0.067943\n",
      "Minibatch accuracy: 0.984375\n",
      "Minibatch loss at step 2350: 0.056500\n",
      "Minibatch accuracy: 0.984375\n",
      "Minibatch loss at step 2400: 0.245044\n",
      "Minibatch accuracy: 0.9375\n",
      "Test accuracy: 0.20459401607513428\n",
      "Minibatch loss at step 2450: 0.131873\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 2500: 0.247952\n",
      "Minibatch accuracy: 0.96875\n",
      "Minibatch loss at step 2550: 0.734723\n",
      "Minibatch accuracy: 0.78125\n",
      "Minibatch loss at step 2600: 1.892577\n",
      "Minibatch accuracy: 0.296875\n",
      "Test accuracy: 0.2975427210330963\n",
      "Minibatch loss at step 2650: 2.838904\n",
      "Minibatch accuracy: 0.375\n",
      "Minibatch loss at step 2700: 0.042575\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 2750: 0.053788\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 2800: 0.092234\n",
      "Minibatch accuracy: 0.984375\n",
      "Test accuracy: 0.23985043168067932\n",
      "Minibatch loss at step 2850: 0.156776\n",
      "Minibatch accuracy: 0.9375\n",
      "Minibatch loss at step 2900: 0.161349\n",
      "Minibatch accuracy: 0.953125\n",
      "Minibatch loss at step 2950: 0.264873\n",
      "Minibatch accuracy: 0.875\n",
      "Minibatch loss at step 3000: 0.171314\n",
      "Minibatch accuracy: 0.96875\n",
      "Test accuracy: 0.2361111044883728\n",
      "Test accuracy: 0.2270299196243286\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "test_dict = {input_data : test_dataset, input_labels : test_labels}\n",
    "             #summary_label: b'validation'}\n",
    "\n",
    "for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {input_data : batch_data, input_labels : batch_labels}\n",
    "#                 summary_label: b'training'}\n",
    "    _, l, predictions, accuracy, summaries, g_step = session.run(\n",
    "        [optimizer, loss, model_prediction, model_accuracy, merged_summaries, inc_step], feed_dict=feed_dict)\n",
    "    if (g_step % 50 == 0):\n",
    "        writer.add_summary(summaries, g_step)\n",
    "        print('Minibatch loss at step %d: %f' % (g_step, l))\n",
    "        print('Minibatch accuracy: {}'.format(accuracy))\n",
    "    if (g_step % 200 == 0):\n",
    "        accuracy, summaries = session.run([model_accuracy, merged_summaries], feed_dict=test_dict)\n",
    "        writer.add_summary(summaries, g_step)\n",
    "        print('Test accuracy: {}'.format(accuracy))\n",
    "        \n",
    "\n",
    "test_accuracy = session.run(model_accuracy, feed_dict=test_dict)\n",
    "print('Test accuracy: {}'.format(test_accuracy))\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For fun, let's inspect some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize data:\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spot_check():\n",
    "    i = np.random.randint(len(test_dataset))\n",
    "    data = test_dataset[i,:]\n",
    "    pixels = np.reshape(data, (image_height, image_width))\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show()\n",
    "    feed_me = np.ndarray((1, image_height * image_width), np.float32)\n",
    "    feed_me[0] = data\n",
    "    feed_dict = {input_data: feed_me}\n",
    "    prediction = session.run(label_prediction, feed_dict=feed_dict)\n",
    "    print(\"Predicted character: \" + chr(prediction + ord('A')))\n",
    "    print(\"Actual label: \" + chr(np.argmax(test_labels[i]) + ord('A')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d8c53bc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted character: E\n",
      "Actual label: E\n"
     ]
    }
   ],
   "source": [
    "spot_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
